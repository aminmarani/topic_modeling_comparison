{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c4fba8-bf8a-483d-9f2d-d269d60e8fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amh418/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/amh418/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/amh418/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amh418/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from diskcache_class import db\n",
    "from lda_mallet import *\n",
    "from pre_processing import *\n",
    "from os import walk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47f6fcb-1be6-4495-9563-15714d7e4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to run this on 4 different dataset 1. EDML 2. AP 3. Newsgroup 4.Covid-Tweet\n",
    "\n",
    "\n",
    "############################## reading data for ED corpus\n",
    "# datafolder = './data/ed_recovery_formatted/Excel'\n",
    "# #datafolder = 'ed_recovery_topicmodel'\n",
    "# df = pd.DataFrame(columns=['url','type','photo','date','tags','notes','text','photo_url','reblogged','blog_name'])\n",
    "\n",
    "# for dirpath,dirnames,filenames in walk(datafolder):\n",
    "#   for filename in filenames:\n",
    "#     if filename.endswith('.xlsx'):\n",
    "#       t = pd.read_excel(datafolder+'/'+filename,names=['url','type','photo','date','tags','notes','text','photo_url','reblogged'])\n",
    "#       blog_name = t.iloc[0,0].split(':')[1]\n",
    "#       t['blog_name'] = blog_name\n",
    "#       df = df.append(t.iloc[3:,:],ignore_index=True)\n",
    "#       print('blog:{0}   with posts:{1}    and reblogs:{2}  '.format(filename,len(t),len(t[t.reblogged=='yes'])))\n",
    "\n",
    "\n",
    "# print('number of blogs: {0} - number of posts: {1}'.format(len(set(df.blog_name)),len(df)))\n",
    "# print('out of {0} documents, {1} are reblogged.'.format(len(df),len(df[df.reblogged == 'yes'])))\n",
    "\n",
    "# #finding reblogged texts\n",
    "# texts = sorted(df.text) #sort them to keep smallest post (perhaps original one) at first\n",
    "# re_texts = []\n",
    "\n",
    "# while len(texts):\n",
    "#   t = [texts.pop(0)]#pop first text and find it!\n",
    "#   if t[0] == ' ' or len(t[0].split())<3: \n",
    "#     continue #almost nothing to look\n",
    "#   i = 0\n",
    "#   while i<len(texts):\n",
    "#     if t[0] in texts[i]:\n",
    "#       t.append(texts.pop(i))\n",
    "#     else:\n",
    "#       i += 1\n",
    "#   if len(t) > 1:\n",
    "#     re_texts.append(t)\n",
    "\n",
    "\n",
    "# print('number of unique reblogged texts: {0}'.format(len(re_texts)))\n",
    "# print('number of unique string in all texts: {0}'.format(len(set(df.text))))\n",
    "\n",
    "# extra_stopwords = ['isnt','want','cant','wanna','im','could','ive','would','dont','get','also','us','thats','got','ur','wanted',\n",
    "#                    'may', 'the', 'just', 'can', 'think', 'damn', 'still', 'guys', 'literally', 'hopefully', 'much', 'even', 'rly', 'guess', 'anon']#anything with a length of one\n",
    "                   \n",
    "\n",
    "# '''pre-processing'''\n",
    "# # original_doc_set = list(df.text[df.photo=='no'])\n",
    "# sel_df = df[df.photo=='no'] #extracting only-text posts\n",
    "# doc_list = list(sel_df.text)\n",
    "\n",
    "\n",
    "# ##############reading AP corpus\n",
    "# text_df = ap_corpus('./data/ap.txt')\n",
    "# doc_list = list(text_df.text_cleaned)\n",
    "\n",
    "##############reading Newsgroup corpus\n",
    "# text_df = newsgroup('./data/20newsgroup_preprocessed.csv')\n",
    "# doc_list = list(text_df.text_cleaned)\n",
    "\n",
    "##############Covid Tweet corpus\n",
    "# doc_list=[]\n",
    "# with open('./data/covid_tweets','r',encoding='utf-8') as txtfile:\n",
    "#     doc_list = txtfile.readlines()\n",
    "\n",
    "##############new ED\n",
    "import pandas as pd\n",
    "doc_list = pd.read_csv('./data/ed_new.csv').text\n",
    "\n",
    "\n",
    "#loading ref corpus for coherene score for lda_mallet\n",
    "# wiki_docs = loading_wiki_docs('./data/wiki_sampled_5p.txt')\n",
    "# # wiki_docs = loading_wiki_docs('g:/wiki_corpus_gensim/wiki_full')\n",
    "# #doing pre-processing on wiki-pedia documents\n",
    "# pre_processed_wiki, _ = preprocess_data(wiki_docs)\n",
    "# wiki_vocab_dict, _ = prepare_corpus(pre_processed_wiki)\n",
    "# del wiki_docs\n",
    "\n",
    "#loading pre-processed wiki docs and associated vocabulary\n",
    "import pickle\n",
    "with open('../wiki_corpus/wiki_full_pre_processed_docs.list','rb') as wfile:\n",
    "    pre_processed_wiki = pickle.load(wfile)\n",
    "    \n",
    "with open('../wiki_corpus/wiki_full_vocab.obj','rb') as wfile:\n",
    "    wiki_vocab_dict = pickle.load(wfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45eb6cb9-e673-426f-a5c6-0a643063d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertopic import BERTopic\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# import pandas as pd\n",
    "# docs = pd.read_csv('./data/ed_new.csv').original_text\n",
    "\n",
    "# topic_model = BERTopic()\n",
    "# topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e27d856-569a-4102-b22b-aab042c74056",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ed_new.txt','w') as tfile:\n",
    "    for s in docs:\n",
    "        tfile.write(s.strip()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb10c5ce-3c68-45d2-bd6b-e44c0a9391f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing terms that are not in Wikipedia ref-corpus\n",
    "pre_processed_docs,filtered_docs = preprocess_data(doc_list,extra_stopwords={})\n",
    "#generate vocabulary and texts\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(pre_processed_docs)\n",
    "\n",
    "#finding stopwords that are not in Wikipedia and removing those\n",
    "extra_stopwords = set(vocab_dict.token2id.keys()).difference(set(wiki_vocab_dict.token2id.keys()))\n",
    "pre_processed_docs,filtered_docs = preprocess_data(doc_list,extra_stopwords=extra_stopwords)\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(pre_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930ed8b-d991-4482-ba7c-d8dd8adb1de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#running many many LDA and storing their pair of terms\n",
    "\n",
    "start = 90; limit=100; step = 10\n",
    "runs = 3\n",
    "term_pairs = set()\n",
    "\n",
    "for num_topics in tqdm(range(start, limit+1, step)):\n",
    "    model_t = []\n",
    "    purity_t = []\n",
    "    coherence_t = []\n",
    "    contrast_t = []\n",
    "    for r in range(runs):\n",
    "        #model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=num_topics, id2word=vocab_dict,optimize_interval = 25,workers=1)\n",
    "\n",
    "        #storing top_terms\n",
    "        for tn in range(num_topics): \n",
    "            tt = model.show_topic(tn,topn=20)\n",
    "\n",
    "            #saving top_terms and their counts\n",
    "            top_terms = [i[0] for i in tt]\n",
    "\n",
    "            #making pair terms\n",
    "            for i in range(len(top_terms)):\n",
    "                for j in range(i+1,len(top_terms)):\n",
    "                    term_pairs.add((top_terms[i],top_terms[j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f53605-4d3c-4191-9215-a0f4fbc1746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting set to list\n",
    "term_pairs_ls = [[i[0],i[1]] for i in list(term_pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b860f009-40b5-40ab-89b8-7009f022cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('term_pairs.obk','wb') as ofile:\n",
    "#     pickle.dump(term_pairs_ls,ofile)\n",
    "\n",
    "import pickle\n",
    "with open('./data/term_pairs.obk','rb') as ofile:\n",
    "    term_pairs_ls = pickle.load(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a66fe6d-1c3e-4ebc-8dc2-b75e02047af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing Coherence for all term pairs\n",
    "cscore = CoherenceModel(topics=term_pairs_ls,dictionary=wiki_vocab_dict,texts=pre_processed_wiki,coherence='c_npmi',processes=3,topn=2).get_coherence_per_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcdbcfe-0672-4bc5-b4ec-bcab6b52fdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NPMI coherence DB. \n",
      "Number of keys : 1934773\n"
     ]
    }
   ],
   "source": [
    "#Loading the DB\n",
    "npmi_db = db('./data/wiki_full/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f22b4c-b73e-4a52-a603-c7b40b951a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158968/158968 [9:50:37<00:00,  4.49it/s]   \n"
     ]
    }
   ],
   "source": [
    "#writing into the DB\n",
    "for i in tqdm(range(len(term_pairs_ls))):\n",
    "    #if the key does not exist, insert it\n",
    "    try:\n",
    "        npmi_db.db[(term_pairs_ls[i][0],term_pairs_ls[i][1])]\n",
    "    except:\n",
    "        npmi_db.db[(term_pairs_ls[i][0],term_pairs_ls[i][1])] = cscore[i]\n",
    "    #do the other combination\n",
    "    try:\n",
    "        npmi_db.db[(term_pairs_ls[i][1],term_pairs_ls[i][0])]\n",
    "    except:\n",
    "        npmi_db.db[(term_pairs_ls[i][1],term_pairs_ls[i][0])] = cscore[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89dff822-9d62-47e9-bf8d-660e5df111e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125087it [00:23, 5268.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# for k in tqdm(npmi_db.db.iterkeys()):\n",
    "#     try:\n",
    "#         npmi_db.db[(k[1],k[0])]\n",
    "#     except:\n",
    "#         npmi_db.db[(k[1],k[0])] =  npmi_db.db[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429c0d1-a093-4059-b24b-bf93b06f7027",
   "metadata": {},
   "source": [
    "## Reading keys from two databases to compute coherence for the difference using Wiki-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25495734-6a37-4d07-ad37-726d7142a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amh418/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/amh418/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/amh418/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amh418/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NPMI coherence DB. \n",
      "Number of keys : 2316260\n"
     ]
    }
   ],
   "source": [
    "from diskcache_class import db\n",
    "from lda_mallet import *\n",
    "from pre_processing import *\n",
    "from os import walk\n",
    "from tqdm import tqdm\n",
    "\n",
    "npmi_db = db('./data/wiki_full/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980741ce-f629-48b2-a2e6-8becaca2087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2316260it [45:42, 844.43it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2316260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_keys = [i for i in npmi_db.db.iterkeys()]\n",
    "new_keys = []\n",
    "for k in tqdm(npmi_db.db.iterkeys()):\n",
    "    new_keys.append(k)\n",
    "len(new_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc56e8c-b669-4a0d-bbc7-a638b10e32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NPMI coherence DB. \n",
      "Number of keys : 1950304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1879397/2316260 [46:17<09:38, 755.25it/s]  "
     ]
    }
   ],
   "source": [
    "keys2search = []\n",
    "npmi_db_old = db('./data/wiki_full_old/')\n",
    "for k in tqdm(new_keys):\n",
    "    if npmi_db_old.get(k)>-100:\n",
    "        keys2search.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea6cc09-0c26-4558-ab6d-c04a169051fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('./data/keys2search.obj','wb') as wfile:\n",
    "#     pickle.dump(keys2search,wfile)\n",
    "\n",
    "with open('./data/keys2search.obj','rb') as wfile:\n",
    "    keys2search = pickle.load(wfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54412b2e-9c19-4b00-be51-b4d6fb192872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pre-processed wiki docs and associated vocabulary\n",
    "import pickle\n",
    "with open('../wiki_corpus/wiki_full_pre_processed_docs.list','rb') as wfile:\n",
    "    pre_processed_wiki = pickle.load(wfile)\n",
    "    \n",
    "with open('../wiki_corpus/wiki_full_vocab.obj','rb') as wfile:\n",
    "    wiki_vocab_dict = pickle.load(wfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6440702f-70d8-4981-8269-af526a16bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cscore = CoherenceModel(topics=[[i[0],i[1]] for i in keys2search if i[0] in wiki_vocab_dict.token2id.keys() and i[1] in wiki_vocab_dict.token2id.keys()],dictionary=wiki_vocab_dict,texts=pre_processed_wiki,coherence='c_npmi',processes=4,topn=2).get_coherence_per_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2883c520-4b8f-4329-844e-7b4e626e10c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/289954 [01:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diskcache/core.py\u001b[0m in \u001b[0;36m_transact\u001b[0;34m(self, retry, filename)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                     \u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BEGIN IMMEDIATE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: cannot start a transaction within a transaction",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/amh418/10975257/ipykernel_133267/2309350095.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys2search\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeys2search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_vocab_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkeys2search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_vocab_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnpmi_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys2search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeys2search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnpmi_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys2search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeys2search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diskcache/core.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_row_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diskcache/core.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value, expire, read, tag, retry)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;31m# need cleanup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m             rows = sql(\n\u001b[1;32m    798\u001b[0m                 \u001b[0;34m'SELECT rowid, filename FROM Cache'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diskcache/core.py\u001b[0m in \u001b[0;36m_transact\u001b[0;34m(self, retry, filename)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                     \u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BEGIN IMMEDIATE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_txn_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c  = 0\n",
    "for i in tqdm(range(len(keys2search))):\n",
    "    if keys2search[i][0] in wiki_vocab_dict.token2id.keys() and keys2search[i][1] in wiki_vocab_dict.token2id.keys():\n",
    "        npmi_db.db[(keys2search[i][0],keys2search[i][1])] = cscore[c]\n",
    "        npmi_db.db[(keys2search[i][1],keys2search[i][0])] = cscore[c]\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b43f92-f513-4358-bb7a-bf7b054b7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cscore.obj','wb') as wfile:\n",
    "    pickle.dump(cscore,wfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a01b003e-4831-4356-8ec5-24b5182ba9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 22222\n",
    "keys2search[i][0] in wiki_vocab_dict.token2id.keys() and keys2search[i][1] in wiki_vocab_dict.token2id.keys()\n",
    "npmi_db.db[(keys2search[i][0],keys2search[i][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3555185-1553-4741-a490-4fd84dbc0f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289905, 289954)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cscore),len(keys2search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
