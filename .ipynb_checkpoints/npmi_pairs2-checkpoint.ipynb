{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aminh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aminh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aminh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aminh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in setting maxSize for CSV output\n"
     ]
    }
   ],
   "source": [
    "from diskcache_class import db\n",
    "from lda_mallet import *\n",
    "from pre_processing import *\n",
    "from os import walk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog:ariessuntaurusrising_formatted.xlsx   with posts:504    and reblogs:206  \n",
      "blog:bagelbells_formatted.xlsx   with posts:1101    and reblogs:1046  \n",
      "blog:brittle-bunny_formatted.xlsx   with posts:121    and reblogs:20  \n",
      "blog:faithhopeloveandtherapy_formatted.xlsx   with posts:2994    and reblogs:1510  \n",
      "blog:fearless-foodie_formatted.xlsx   with posts:3956    and reblogs:3670  \n",
      "blog:finallyrecoveringforgood_formatted.xlsx   with posts:137    and reblogs:49  \n",
      "blog:fuck-eatingdisorders_formatted.xlsx   with posts:37    and reblogs:4  \n",
      "blog:hellsite-residential_formatted.xlsx   with posts:247    and reblogs:191  \n",
      "blog:imtrying-butimpissed_formatted.xlsx   with posts:838    and reblogs:483  \n",
      "blog:intentandotrascender_formatted.xlsx   with posts:21    and reblogs:0  \n",
      "blog:lets-get-better_formatted.xlsx   with posts:509    and reblogs:453  \n",
      "blog:matchayogurt_formatted.xlsx   with posts:219    and reblogs:168  \n",
      "blog:mirithebrave_formatted.xlsx   with posts:578    and reblogs:299  \n",
      "blog:mote-of-dust_formatted.xlsx   with posts:271    and reblogs:238  \n",
      "blog:palpitationstation_formatted.xlsx   with posts:1439    and reblogs:888  \n",
      "blog:rec-hovery_formatted.xlsx   with posts:22    and reblogs:7  \n",
      "blog:shameofateen_formatted.xlsx   with posts:54    and reblogs:2  \n",
      "number of blogs: 17 - number of posts: 12997\n",
      "out of 12997 documents, 9234 are reblogged.\n",
      "number of unique reblogged texts: 234\n",
      "number of unique string in all texts: 11473\n"
     ]
    }
   ],
   "source": [
    "#we are going to run this on 4 different dataset 1. EDML 2. AP 3. Newsgroup 4.Covid-Tweet\n",
    "\n",
    "\n",
    "############################## reading data for ED corpus\n",
    "datafolder = './data/ed_recovery_formatted/Excel'\n",
    "#datafolder = 'ed_recovery_topicmodel'\n",
    "df = pd.DataFrame(columns=['url','type','photo','date','tags','notes','text','photo_url','reblogged','blog_name'])\n",
    "\n",
    "for dirpath,dirnames,filenames in walk(datafolder):\n",
    "  for filename in filenames:\n",
    "    if filename.endswith('.xlsx'):\n",
    "      t = pd.read_excel(datafolder+'/'+filename,names=['url','type','photo','date','tags','notes','text','photo_url','reblogged'],engine='openpyxl')\n",
    "      blog_name = t.iloc[0,0].split(':')[1]\n",
    "      t['blog_name'] = blog_name\n",
    "      df = df.append(t.iloc[3:,:],ignore_index=True)\n",
    "      print('blog:{0}   with posts:{1}    and reblogs:{2}  '.format(filename,len(t),len(t[t.reblogged=='yes'])))\n",
    "\n",
    "\n",
    "print('number of blogs: {0} - number of posts: {1}'.format(len(set(df.blog_name)),len(df)))\n",
    "print('out of {0} documents, {1} are reblogged.'.format(len(df),len(df[df.reblogged == 'yes'])))\n",
    "\n",
    "#finding reblogged texts\n",
    "texts = sorted(df.text) #sort them to keep smallest post (perhaps original one) at first\n",
    "re_texts = []\n",
    "\n",
    "while len(texts):\n",
    "  t = [texts.pop(0)]#pop first text and find it!\n",
    "  if t[0] == ' ' or len(t[0].split())<3: \n",
    "    continue #almost nothing to look\n",
    "  i = 0\n",
    "  while i<len(texts):\n",
    "    if t[0] in texts[i]:\n",
    "      t.append(texts.pop(i))\n",
    "    else:\n",
    "      i += 1\n",
    "  if len(t) > 1:\n",
    "    re_texts.append(t)\n",
    "\n",
    "\n",
    "print('number of unique reblogged texts: {0}'.format(len(re_texts)))\n",
    "print('number of unique string in all texts: {0}'.format(len(set(df.text))))\n",
    "\n",
    "extra_stopwords = ['isnt','want','cant','wanna','im','could','ive','would','dont','get','also','us','thats','got','ur','wanted',\n",
    "                   'may', 'the', 'just', 'can', 'think', 'damn', 'still', 'guys', 'literally', 'hopefully', 'much', 'even', 'rly', 'guess', 'anon']#anything with a length of one\n",
    "                   \n",
    "\n",
    "'''pre-processing'''\n",
    "# original_doc_set = list(df.text[df.photo=='no'])\n",
    "sel_df = df[df.photo=='no'] #extracting only-text posts\n",
    "doc_list = list(sel_df.text)\n",
    "\n",
    "\n",
    "# ##############reading AP corpus\n",
    "# text_df = ap_corpus('./data/ap.txt')\n",
    "# doc_list = list(text_df.text_cleaned)\n",
    "\n",
    "##############reading Newsgroup corpus\n",
    "# text_df = newsgroup('./data/20newsgroup_preprocessed.csv')\n",
    "# doc_list = list(text_df.text_cleaned)\n",
    "\n",
    "##############Covid Tweet corpus\n",
    "# doc_list=[]\n",
    "# with open('./data/covid_tweets','r',encoding='utf-8') as txtfile:\n",
    "#     doc_list = txtfile.readlines()\n",
    "\n",
    "\n",
    "#loading ref corpus for coherene score for lda_mallet\n",
    "wiki_docs = loading_wiki_docs('./data/wiki_sampled_10p.txt')\n",
    "#doing pre-processing on wiki-pedia documents\n",
    "pre_processed_wiki, _ = preprocess_data(wiki_docs)\n",
    "wiki_vocab_dict, _ = prepare_corpus(pre_processed_wiki)\n",
    "del wiki_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing terms that are not in Wikipedia ref-corpus\n",
    "pre_processed_docs,filtered_docs = preprocess_data(doc_list,extra_stopwords={})\n",
    "#generate vocabulary and texts\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(pre_processed_docs)\n",
    "\n",
    "#finding stopwords that are not in Wikipedia and removing those\n",
    "extra_stopwords = set(vocab_dict.token2id.keys()).difference(set(wiki_vocab_dict.token2id.keys()))\n",
    "pre_processed_docs,filtered_docs = preprocess_data(doc_list,extra_stopwords=extra_stopwords)\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(pre_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 8/8 [8:53:50<00:00, 4003.83s/it]\n"
     ]
    }
   ],
   "source": [
    "#running many many LDA and storing their pair of terms\n",
    "import numpy as np\n",
    "\n",
    "start = 6; limit=13; step = 1\n",
    "runs = 3\n",
    "num_permutation = 100\n",
    "iterations = 5000\n",
    "term_pairs = set()\n",
    "\n",
    "for num_topics in tqdm(range(start, limit+1, step)):\n",
    "    model_t = []\n",
    "    purity_t = []\n",
    "    coherence_t = []\n",
    "    contrast_t = []\n",
    "    for iter in np.random.permutation(iterations).tolist()[0:num_permutation]:\n",
    "        #model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=num_topics, id2word=vocab_dict,optimize_interval = 25,workers=1, iterations=iter)\n",
    "\n",
    "        #storing top_terms\n",
    "        for tn in range(num_topics): \n",
    "            tt = model.show_topic(tn,topn=20)\n",
    "\n",
    "            #saving top_terms and their counts\n",
    "            top_terms = [i[0] for i in tt]\n",
    "\n",
    "            #making pair terms\n",
    "            for i in range(len(top_terms)):\n",
    "                for j in range(i+1,len(top_terms)):\n",
    "                    term_pairs.add((top_terms[i],top_terms[j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting set to list\n",
    "term_pairs_ls = [[i[0],i[1]] for i in list(term_pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing Coherence for all term pairs\n",
    "cscore = CoherenceModel(topics=term_pairs,dictionary=wiki_vocab_dict,texts=pre_processed_wiki,coherence='c_npmi',processes=3,topn=2).get_coherence_per_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NPMI coherence DB. \n",
      "Number of keys : 187837\n"
     ]
    }
   ],
   "source": [
    "#Loading the DB\n",
    "npmi_db = db('./db/wiki_10p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 42554/42554 [00:13<00:00, 3193.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#writing into the DB\n",
    "for i in tqdm(range(len(term_pairs_ls))):\n",
    "    #if the key does not exist, insert it\n",
    "    try:\n",
    "        npmi_db.db[(term_pairs_ls[i][0],term_pairs_ls[i][1])]\n",
    "    except:\n",
    "        npmi_db.db[(term_pairs_ls[i][0],term_pairs_ls[i][1])] = cscore[i]\n",
    "    #do the other combination\n",
    "#     try:\n",
    "#         npmi_db.db[(term_pairs_ls[i][1],term_pairs_ls[i][0])]\n",
    "#     except:\n",
    "#         npmi_db.db[(term_pairs_ls[i][1],term_pairs_ls[i][0])] = cscore[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating new NPMIs for a new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NPMI coherence DB. \n",
      "Number of keys : 364450\n",
      "Load NPMI coherence DB. \n",
      "Number of keys : 216712\n"
     ]
    }
   ],
   "source": [
    "old_db = db('./db/wiki_5p_old')\n",
    "new_db = db('./db/wiki_10p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a9127242f228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#doing pre-processing on wiki-pedia documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpre_processed_wiki\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwiki_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwiki_vocab_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processed_wiki\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mwiki_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\topic_modeling_comparison\\pre_processing.py\u001b[0m in \u001b[0;36mprepare_corpus\u001b[1;34m(doc_clean, no_below, no_above)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_below\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_above\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mdoc_term_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_clean\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;31m# generate LDA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\topic_modeling_comparison\\pre_processing.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_below\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_above\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mdoc_term_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_clean\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;31m# generate LDA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#loading ref corpus for coherene score for lda_mallet\n",
    "abs_path = 'G:\\wiki_corpus_gensim\\wiki20p0'\n",
    "wiki_docs = loading_wiki_docs(abs_path)\n",
    "#doing pre-processing on wiki-pedia documents\n",
    "pre_processed_wiki, _ = preprocess_data(wiki_docs)\n",
    "wiki_vocab_dict, _ = prepare_corpus(pre_processed_wiki)\n",
    "del wiki_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 60000/60000 [00:08<00:00, 6806.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 60000 NPMI into database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 60000/60000 [00:07<00:00, 7736.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 60000 NPMI into database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 60000/60000 [00:14<00:00, 4012.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 60000 NPMI into database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 37820/37820 [00:15<00:00, 2400.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 37820 remaining NPMI into database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "term_pairs = set()\n",
    "max_ins = 60000\n",
    "count = 0\n",
    "\n",
    "for k in old_db.db.iterkeys():\n",
    "    #check if it is already in the list but in reverse order + if it is in the vocab\n",
    "    if (k[1],k[0]) not in term_pairs and k[0] in wiki_vocab_dict.token2id.keys() and k[1] in wiki_vocab_dict.token2id.keys():\n",
    "        #check if it is not in the new db as well\n",
    "        try:\n",
    "            new_db.db((k[0],k[1]))\n",
    "        except:#only add if it is not in the db in the reverse order\n",
    "            term_pairs.add((k[0],k[1]))\n",
    "            count +=1\n",
    "    #compute Gensim and insert into db\n",
    "    if count>=max_ins:\n",
    "        count = 0\n",
    "        #compute coherence\n",
    "        term_pairs_ls = [[i[0],i[1]] for i in list(term_pairs)]\n",
    "        cscore = CoherenceModel(topics=term_pairs,dictionary=wiki_vocab_dict,texts=pre_processed_wiki,coherence='c_npmi',processes=3,topn=2).get_coherence_per_topic()\n",
    "        #inseert into db\n",
    "        for i in tqdm(range(len(term_pairs_ls))):\n",
    "            #if the key does not exist, insert it\n",
    "            try:\n",
    "                skeys = sorted(term_pairs_ls[i])\n",
    "                new_db.db[(skeys[0],skeys[1])]\n",
    "            except:\n",
    "                new_db.db[(skeys[0],skeys[1])] = cscore[i]\n",
    "        print('inserted {0} NPMI into database'.format(max_ins))\n",
    "        term_pairs = set()\n",
    "        \n",
    "        \n",
    "if count>0:\n",
    "    term_pairs_ls = [[i[0],i[1]] for i in list(term_pairs)]\n",
    "    cscore = CoherenceModel(topics=term_pairs,dictionary=wiki_vocab_dict,texts=pre_processed_wiki,coherence='c_npmi',processes=3,topn=2).get_coherence_per_topic()\n",
    "    #inseert into db\n",
    "    for i in tqdm(range(len(term_pairs_ls))):\n",
    "        #if the key does not exist, insert it\n",
    "        try:\n",
    "            skeys = sorted(term_pairs_ls[i])\n",
    "            new_db.db[(skeys[0],skeys[1])]\n",
    "        except:\n",
    "            new_db.db[(skeys[0],skeys[1])] = cscore[i]\n",
    "    print('inserted {0} remaining NPMI into database'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.tmp','w') as txtfile:\n",
    "    for t in term_pairs_ls:\n",
    "        txtfile.write(t[0]+t[1]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
