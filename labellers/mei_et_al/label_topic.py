import numpy as np
from timeit import default_timer as timer

from sklearn.feature_extraction.text import (CountVectorizer
                                             as WordCountVectorizer)
import sys
sys.path.append('labellers/')
from mei_et_al.text import LabelCountVectorizer
from mei_et_al.label_finder import BigramLabelFinder
from mei_et_al.label_ranker import LabelRanker
from mei_et_al.pmi import PMICalculator
from mei_et_al.corpus_processor import (CorpusStopwordFilter,
                                        CorpusWordLengthFilter,
                                        CorpusPOSTagger,
                                        CorpusStemmer)
from mei_et_al.data import (load_stopwords, tokenize_docs)
    
"""
model - Model of topics represented as word probabilities over the topics
docs - List where each element is a document
vocabulary - Vocabulary generated by mallet
stopwords - String giving filename of document that contains stopwords
preprocessing_steps - List of strings that correspond with different preprocessing methods (Current options: 'stopword', 'wordlen', 'stem', 'tag')
n_cand_labels - Maximum number of candidate labels to generate.
label_min_df - Minimum number of occurences of word to appear in a label
labels_per_topic - Number of labels to show per topic
label_tags - When 'tag' is in preprocessing_steps, this is used further specify what form the bigrams should be (uses nltk to tag words)
"""
def get_topic_labels(model,
                     docs, 
                     vocabulary, 
                     stopwords,
                     preprocessing_steps,
                     n_cand_labels, 
                     label_min_df,
                     labels_per_topic,
                     label_tags = [],
                     delim = ', '):
    
    n_cand_labels = int(n_cand_labels)
    label_min_df = int(label_min_df)
    labels_per_topic = int(labels_per_topic)
    stopwordslist = []
    
    print("Tokenizing documents...")
    sys.stdout.flush()
    start = timer()
    tokenized_docs = tokenize_docs(docs)
    documents = tokenized_docs
    end = timer()
    print("" + str(end - start) + " seconds to tokenize")
    start = end
    sys.stdout.flush()
    
    if 'stopword' in preprocessing_steps:
        stopwordslist = load_stopwords("../../" + stopwords)
        print("Stopword filtering...")
        sys.stdout.flush()
        wl_filter = CorpusStopwordFilter(stopwordslist)
        tokenized_docs = list(wl_filter.transform(tokenized_docs))
        end = timer()
        print("Complete!: " + str(end - start) + " seconds to stopword filter")
        start = end
        sys.stdout.flush()
    
    if 'wordlen' in preprocessing_steps:
        print("Word length filtering...")
        sys.stdout.flush()
        wl_filter = CorpusWordLengthFilter(minlen=2)
        tokenized_docs = list(wl_filter.transform(tokenized_docs))
        end = timer()
        print("Complete!: " + str(end - start) + " seconds to word length filter")
        start = end
        sys.stdout.flush()
    
    if 'stem' in preprocessing_steps:
        print("Stemming...")
        sys.stdout.flush()
        stemmer = CorpusStemmer()
        tokenized_docs = list(stemmer.transform(tokenized_docs))
        end = timer()
        print("Complete!: " + str(end - start) + " seconds to stem")
        start = end
        sys.stdout.flush()
    
    if 'tag' in preprocessing_steps:
        print("POS tagging...")
        sys.stdout.flush()
        tagger = CorpusPOSTagger()
        tagged_docs = list(tagger.transform(tokenized_docs))
        end = timer()
        print("Complete!: " + str(end - start) + " seconds to POS tag")
        start = end
        sys.stdout.flush()
    
    tag_constraints = []
    if label_tags != ['None']:
        for tags in label_tags:
            tag_constraints.append(tuple(map(lambda t: t.strip(),
                                             tags.split(','))))

    if len(tag_constraints) == 0 or not "tag" in preprocessing_steps:
        tag_constraints = None
    else:
        print("Tag constraints: {}".format(tag_constraints))

    print("Generate candidate bigram labels...")
    sys.stdout.flush()
    finder = BigramLabelFinder('chi_sq', min_freq=label_min_df, #options: 'pmi', 'chi_sq', or 'student_t'
                               pos=tag_constraints)
    
    start = timer()
    if tag_constraints:
        assert 'tag' in preprocessing_steps, \
            'If tag constraint is applied, pos tagging(tag) should be performed'
        cand_labels = list(finder.find(tagged_docs, top_n=n_cand_labels))
    else:  # if no constraint, then use untagged tokenized_docs
        cand_labels = list(finder.find(tokenized_docs, top_n=n_cand_labels))
    end = timer()
    print("Complete!: " + str(end - start) + " seconds to find candidate labels")
    start = end
    sys.stdout.flush()
    
    print("Collected {} candidate labels".format(len(cand_labels)))
    
    print("Calculate the PMI scores...")
    sys.stdout.flush()
    
    start = timer()
    pmi_cal = PMICalculator(
        doc2word_vectorizer=WordCountVectorizer(
            min_df=1,
            stop_words=stopwordslist,
            token_pattern=r"[^ ]+", #Use if need to generate vocab: r"(?u)\b[^\W_][\w\\_/&'-]+[^\W\d_]\b"
            vocabulary=vocabulary),
        doc2label_vectorizer=LabelCountVectorizer())
    
    pmi_w2l = pmi_cal.from_texts(tokenized_docs = documents, labels = cand_labels, pseudo_count = 3000/len(vocabulary)) #pseudo_count: base co-occurrence of words and labels to prevent 0s
    
    end = timer()
    print("Complete!: " + str(end - start) + " seconds to calculate PMI scores")
    start = end
    sys.stdout.flush()

    """ Necessary if vocabulary isn't enforced on countvectorizer: 
    print("Rearranging columns of model to match vocabulary...")
    start = timer()
    shift = []
    for word in vocabulary:
        shift.append(pmi_cal._d2w_vect.vocabulary_[word])
    shift = np.argsort(shift)
    model = model[:,shift]
    end = timer()
    print("Complete!: " + str(end - start) + " seconds")
    start = end
    sys.stdout.flush()
    """
    
    ranker = LabelRanker(apply_intra_topic_coverage=False, mu=(21/(model.shape[0]+30) + .3)) #mu affects how much the discriminative score affects total [0,1]
    
    print("Model word count: " + str(model.shape[1]) + "   -   PMI word count: " + str(pmi_w2l.shape[0]))
    
    sys.stdout.flush()
    
    start = timer()
    labels = list(map(lambda l: delim.join(map(lambda t:' '.join(t), l)), ranker.top_k_labels(topic_models=model,
                               pmi_w2l=pmi_w2l,
                               index2label=pmi_cal.index2label_,
                               label_models=None,
                               k=labels_per_topic)))
    
    end = timer()
    print("Complete!: " + str(end - start) + " seconds to rank labels")
    start = end
    sys.stdout.flush()

    return labels
