{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02e6ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/das-\n",
      "[nltk_data]     lab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/das-lab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/das-lab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/das-lab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog:mote-of-dust_formatted.xlsx   with posts:271    and reblogs:238  \n",
      "blog:fuck-eatingdisorders_formatted.xlsx   with posts:37    and reblogs:4  \n",
      "blog:hellsite-residential_formatted.xlsx   with posts:247    and reblogs:191  \n",
      "blog:lets-get-better_formatted.xlsx   with posts:509    and reblogs:453  \n",
      "blog:bagelbells_formatted.xlsx   with posts:1101    and reblogs:1046  \n",
      "blog:imtrying-butimpissed_formatted.xlsx   with posts:838    and reblogs:483  \n",
      "blog:faithhopeloveandtherapy_formatted.xlsx   with posts:2994    and reblogs:1510  \n",
      "blog:finallyrecoveringforgood_formatted.xlsx   with posts:137    and reblogs:49  \n",
      "blog:intentandotrascender_formatted.xlsx   with posts:21    and reblogs:0  \n",
      "blog:fearless-foodie_formatted.xlsx   with posts:3956    and reblogs:3670  \n",
      "blog:ariessuntaurusrising_formatted.xlsx   with posts:504    and reblogs:206  \n",
      "blog:rec-hovery_formatted.xlsx   with posts:22    and reblogs:7  \n",
      "blog:shameofateen_formatted.xlsx   with posts:54    and reblogs:2  \n",
      "blog:brittle-bunny_formatted.xlsx   with posts:121    and reblogs:20  \n",
      "blog:mirithebrave_formatted.xlsx   with posts:578    and reblogs:299  \n",
      "blog:matchayogurt_formatted.xlsx   with posts:219    and reblogs:168  \n",
      "blog:palpitationstation_formatted.xlsx   with posts:1439    and reblogs:888  \n",
      "number of blogs: 17 - number of posts: 12997\n",
      "out of 12997 documents, 9234 are reblogged.\n",
      "number of unique reblogged texts: 234\n",
      "number of unique string in all texts: 11473\n",
      "size of orginal dataset: 8276 and size of the pre-processed dataset: 1\n",
      "Vocab size before lemmatiziation: 0 and after lemmatization: 0\n"
     ]
    }
   ],
   "source": [
    "from pre_processing import *\n",
    "from lda_mallet import *\n",
    "\n",
    "from os import walk\n",
    "\n",
    "\n",
    "#reading data\n",
    "datafolder = './data/ed_recovery_formatted/Excel'\n",
    "#datafolder = 'ed_recovery_topicmodel'\n",
    "df = pd.DataFrame(columns=['url','type','photo','date','tags','notes','text','photo_url','reblogged','blog_name'])\n",
    "\n",
    "for dirpath,dirnames,filenames in walk(datafolder):\n",
    "  for filename in filenames:\n",
    "    if filename.endswith('.xlsx'):\n",
    "      t = pd.read_excel(datafolder+'/'+filename,names=['url','type','photo','date','tags','notes','text','photo_url','reblogged'])\n",
    "      blog_name = t.iloc[0,0].split(':')[1]\n",
    "      t['blog_name'] = blog_name\n",
    "      df = df.append(t.iloc[3:,:],ignore_index=True)\n",
    "      print('blog:{0}   with posts:{1}    and reblogs:{2}  '.format(filename,len(t),len(t[t.reblogged=='yes'])))\n",
    "\n",
    "\n",
    "print('number of blogs: {0} - number of posts: {1}'.format(len(set(df.blog_name)),len(df)))\n",
    "print('out of {0} documents, {1} are reblogged.'.format(len(df),len(df[df.reblogged == 'yes'])))\n",
    "\n",
    "#finding reblogged texts\n",
    "texts = sorted(df.text) #sort them to keep smallest post (perhaps original one) at first\n",
    "re_texts = []\n",
    "\n",
    "while len(texts):\n",
    "  t = [texts.pop(0)]#pop first text and find it!\n",
    "  if t[0] == ' ' or len(t[0].split())<3: \n",
    "    continue #almost nothing to look\n",
    "  i = 0\n",
    "  while i<len(texts):\n",
    "    if t[0] in texts[i]:\n",
    "      t.append(texts.pop(i))\n",
    "    else:\n",
    "      i += 1\n",
    "  if len(t) > 1:\n",
    "    re_texts.append(t)\n",
    "\n",
    "\n",
    "print('number of unique reblogged texts: {0}'.format(len(re_texts)))\n",
    "print('number of unique string in all texts: {0}'.format(len(set(df.text))))\n",
    "\n",
    "extra_stopwords = ['isnt','want','cant','wanna','im','could','ive','would','dont','get','also','us','thats','got','ur','wanted',\n",
    "                   'may', 'the', 'just', 'can', 'think', 'damn', 'still', 'guys', 'literally', 'hopefully', 'much', 'even', 'rly', 'guess', 'anon']#anything with a length of one\n",
    "                   \n",
    "\n",
    "'''pre-processing'''\n",
    "original_doc_set = list(df.text[df.photo=='no'])\n",
    "pre_processed_docs,filtered_docs = preprocess_data(original_doc_set,extra_stopwords=extra_stopwords)\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(pre_processed_docs)\n",
    "\n",
    "print('size of orginal dataset: {0} and size of the pre-processed dataset: {1}'.format(len(original_doc_set),len(pre_processed_docs)))\n",
    "\n",
    "pre_processed_docs_lem,filtered_docs_lem = preprocess_data(original_doc_set,extra_stopwords=extra_stopwords,lemmatized=True)\n",
    "vocab_dict_lem, doc_term_matrix_lem = prepare_corpus(pre_processed_docs_lem)\n",
    "\n",
    "print('Vocab size before lemmatiziation: {0} and after lemmatization: {1}'.format(len(vocab_dict),len(vocab_dict_lem)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae102447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/das-\n",
      "[nltk_data]     lab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/das-lab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/das-lab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/das-lab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from pre_processing import *\n",
    "#loading the dataset we train the model on\n",
    "text_df = newsgroup('./data/20newsgroup_preprocessed.csv')\n",
    "doc_list = list(text_df.text_cleaned)\n",
    "pre_processed_docs,filtered_docs = preprocess_data(doc_list)\n",
    "vocab_dict_, doc_term_matrix_ = prepare_corpus(pre_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d66250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13588"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_dict_['existence']\n",
    "vocab_dict_.token2id['ax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00627692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading reference corpus\n",
    "wiki_docs = loading_wiki_docs('./data/wiki_sampled_5p.txt')\n",
    "pre_processed_wiki,no_var = preprocess_data(wiki_docs)\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(pre_processed_wiki)\n",
    "\n",
    "#loading the dataset we train the model on\n",
    "text_df = newsgroup('./data/20newsgroup_preprocessed.csv')\n",
    "doc_list = list(text_df.text_cleaned)\n",
    "pre_processed_docs,filtered_docs = preprocess_data(doc_list)\n",
    "vocab_dict_, doc_term_matrix_ = prepare_corpus(pre_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce3bf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda_mallet import *\n",
    "\n",
    "def reading_results(res,topic_num,itreations):\n",
    "  '''\n",
    "  Reading result to get top terms and log-likelihood every 50 iterations\n",
    "\n",
    "  returns: Log-Likelihood and all_top terms for each 50 iterations\n",
    "\n",
    "  parameter res: all the command prompt prints and results (type:str)\n",
    "  '''\n",
    "\n",
    "  #reading the string\n",
    "  current_line = 9#where to start in CSV file\n",
    "  stp1 = topic_num+2#where to read Log-Likelihood\n",
    "  stp2 = 6 #constant number of lines to get to topics again\n",
    "\n",
    "  #please set the number of iteration to what you set for your topic model\n",
    "  all_top_terms = []#storing all top terms\n",
    "  LLs = []\n",
    "  for _i in range(int(itreations/50)):\n",
    "    top_terms = []\n",
    "    for i in range(topic_num):#reading top \n",
    "      try:\n",
    "        #reading top terms splited by two tabs + the second split is for splitting the terms with space\n",
    "        #excluding the last item using [0:-1].  because the last item is '\\n'\n",
    "        top_terms.append(res[current_line].split('\\t')[2].split(' ')[0:-1])\n",
    "      except:\n",
    "        print(res[current_line-2:current_line+3],current_line)\n",
    "      current_line+=1#going to next line\n",
    "    current_line+=1 #going to LL\n",
    "    if _i>3: #optimizing alpha would add [beta] update after 250 iterations and we want to add one line for that\n",
    "      current_line +=1\n",
    "    LLs.append(float(res[current_line].split(': ')[1]))\n",
    "    current_line += stp2\n",
    "    all_top_terms.append(top_terms)\n",
    "\n",
    "  return all_top_terms,LLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6643640",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t.csv','r') as csvfile:\n",
    "    res = csvfile.readlines()\n",
    "#   res = res.split('\\n')\n",
    "all_top_terms,LLs = reading_results(res,3,250)\n",
    "coherence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee5039a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18022"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict.token2id['ax']\n",
    "# all_top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f732151",
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_terms in all_top_terms:\n",
    "    coherence.append(CoherenceModel(topics=top_terms,dictionary=vocab_dict,texts=pre_processed_wiki,topn=5,coherence='c_npmi').get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "694dfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CoherenceModel.for_topics(topics_as_topn_terms=all_top_terms,dictionary=vocab_dict,texts=pre_processed_wiki,topn=5,coherence='c_npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36235e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
